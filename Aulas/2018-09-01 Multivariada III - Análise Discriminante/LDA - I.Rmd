---
title: "LDA - I"
author: "Felipe N. S. Bezerra"
date: "15 de setembro de 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

LDA - Linear Discriminant Analysis
J van Melis
2 de setembro de 2018
Analise de Discriminantes Lineares (ou LDA - Linear Discriminant Analysis)
Introdução geral
A LDA é uma técnica clássica de análise multivariada, que tem como principais aplicações:

Discriminar e Classificar objetos em grupos pré-definidos (2 ou mais grupos)
Definir se os meus grupos conseguem ser bem distintos para os objetos analisados
Encontrar o poder discriminatório dentre as variáveis, verificando a importância que uma variável
A Regressão Logística também tem o objetivo de usar variáveis independentes e produzir como resposta o grupo a qual um objeto pertence (1 ou 0, p.ex), mas a LDA:

Usa regressão linear mas é interpretada de maneira diferente (Grupos ~ X1 + X2b&Xn), pois os beixos são simplificadosb (Funções Discriminantes que são construídas a partir das variáveis)
Funciona para separar 2 ou mais grupos.
A LDA e Análises de Agrupamentos (Clustering) são semelhantes, pois tem como objetivo a separação dos objetos em grupos, mas:

Ao contrário do clustering, na LDA os grupos já são conhecidos (clustering é útil para encontrar esses grupos, baseando-se na similaridade entre as observações);
Pode ser usado para classificar novas observações que contenham as mesmas variáveis utilizadas que foram utilizadas para construir as Funções Discriminates (clustering faz a classificação para o que foi observado).
E, finalmente, a LDA é diferente da Análise de Componentes Principais pois, apesar de haver a construção de uma nova variável que simplifica as variáveis independentes analisadas, a LDA:

É mais interessada nos grupos e não em estudar as variáveis independentes;
É focada em maximizar a separação de grupos conhecidos, enquanto que a PCA foca em reduzir as dimensões dos dados;
O total de variáveis construídas será sempre o número de grupos - 1.
Portanto, a LDA tem como principais objetivos:

Observar se existem diferenças de grupos em perfil multivariado;
Escolher variáveis independentes que ajudem a explicar o máximo de diferenças nos escores dos grupos;
Estabelecer procedimentos para classificar objetos basendo-se em um conjunto de variáveis independentes.


**Pressupostos da LDA**
Os principais pressupostos para essa análise são:

Deve ter os grupos bem estabelecidos;
As variáveis independentes devem ser métricas (quantitativas);
Essas variáveis independentes devem seguir distribuição normal (mas cuidados podem ser feitos);
As variáveis devem mostrar-se com linearidades;
Colinearidades muito altas devem ser evitadas;
Matrizes de variância-covariância iguais entre os grupos;
O tamanho da amostra deve ser considerável [ a) mínimo 5 observações por variável independente; b) cada grupo deve ter mais objetos que variáveis independentes analisadas, c) grupos com tamanhos semelhantes).


**LDA passo-a-passo**
Para a execução de uma LDA, sabendo claramente de qual é seu objetivo, o pesquisador deve seguir 6 passos.

Tamanho da Amostra e Criação de amostras btreinob e btesteb

Seleção e Análise das variáveis independentes

Estimação e Avaliação das Funções Discriminantes

Interpretação das Funções Discriminantes

Validação Cruzada

Conclusões



A seguir, executaremos esses 6 passos com o banco de dados de iris:

```{r}
data("iris")
```

e usaremos a função lda do pacote MASS. A saber que ainda existem os pacotes ade4 e candisc que podem executar LDA.

```{r}
if(!require(MASS)){install.packages('MASS')}
```

1. Consideração quanto ao tamanho da Amostra

Primeiramente, verifique quantos objetos tem em cada grupo:

```{r}
table(iris$Species)
iris
```

Como mantém um número razoável por grupo (50) e só temos 4 variáveis independentes, podemos fazer uma divisão 50:50 para amostras btreinob e btesteb, ou até um pouco maior para btreinob e o restante para btesteb (2:1, por exemplo). Para isso, farei um sorteio de 2/3 do total para btreinob e o restante (1/3) para btesteb:

```{r}
set.seed(51)
n<-length(iris$Species) # quantos objetos existem
sorteio <- sample(1:n, size=n/3)
teste <- iris[-sorteio,]
treino<- iris[sorteio,]
table(teste$Species)
```

```{r}
table(treino$Species)
```

As proporções ficaram um pouco diferentes, se quiser testar se as proporções são semelhantes, você pode aplicar um teste de proporções com ??2:

Para teste:

```{r}
n.teste<-as.numeric(table(teste$Species))
teste.prop<-n.teste/100
prop.test(teste.prop, rep(100/3, 3))
```

Para treino:

```{r}
n.treino<-as.numeric(table(treino$Species))
treino.prop<-n.treino/sum(n.treino)
prop.test(treino.prop, rep(sum(n.treino)/3, 3))
```

Ambas as proporções seguem a distribuição nula (as proporções são iguais com as proporções originais, pois para ambos os p-value foram maiores que 0.05).

2. Seleção de variáveis independentes
Caso você tenha muitas variáveis (mais do que tem de observações em cada grupo), é interessante reduzir a quantidade de variáveis a serem utilizadas. O procedimeto a ser utilizado pode ser por avaliação das variáveis por seus pressupostos e correlações, mas também pode ser utilizado o procedimento de stepwise.

Para críticas e sugestões para efetuar uma LDA com stepwise com o R, sugiro a leitura de alguns links, mas resumidamente podemos considerar:

Quando existem muitas variáveis preditoras.

Geralmente o conjunto reduzido é tão bom - ou melhor - que a totalidade das variáveis.

Émenos estável e generalizável a medida que a proporção entre amostra e variável independente é menor que 20.

Selecionar variáveis independentes em uma modelagem sem ter nenhuma hipótese a priori pode levar a falácias lógicas ou correlações espúrias (além de outros erros).

1 Kozak, M., & Azevedo, R. (2011). Does using stepwise variable selection to build sequential path analysis models make sense? Physiologia plantarum, 141(3), 197-200.

2 Whittingham, M. J., Stephens, P., Bradbury, R. B., & Freckleton, R. P. (2006). Why do we still use stepwise modelling in ecology and behaviour? The Journal of animal ecology, 75(5), 1182-9.

3 Pacote klaR com a função stepclass()

No exemplo de iris, só temos 4 variáveis: Sepal.Length, Sepal.Width, Petal.Length e Petal.Width, que são o comprimento (length) e largura (width) das sépalas (estuturas coloridas mais externas dessa flor, sepal) e pétalas (estruturas mais internas, que são menores que as sépalas nesse gênero de flores, petal).

A seguir, exploraremos os pressupostos das variáveis independentes: (A) Normalidade , (B) Linearidade das relações, (é Multicolinearidade, (D) Igualdade das Matrizes de dispersão entre grupos:

A. Normalidade das variáveis independentes
Podemos utilizar de análise visual/gráfica (QQ-Plot) e/ou estatística (Teste de Shapiro-Wilk) para isso:

Testando a normalidade da variável Sepal.Length para cada espécie de iris:

```{r}
tapply(treino$Sepal.Length, 
         treino$Species, 
         shapiro.test)
```

Testando a normalidade da variável Sepal.Width para cada espécie de iris

```{r}
tapply(treino$Sepal.Width, 
         treino$Species, 
         shapiro.test)
```

Testando a normalidade da variável Petal.Length para cada espécie de iris

```{r}
tapply(treino$Petal.Length, 
         treino$Species, 
         shapiro.test)
```

Testando a normalidade da variável Petal.Width para cada espécie de iris

```{r}
tapply(treino$Petal.Width, 
         treino$Species, 
         shapiro.test)
```

QQ-plot de todas as variáveis, nas linhas são as espécies (1 a 3) e nas colunas as variáveis (de 1 a 4):

```{r}
par(mfrow=c(3,4))
species<-levels(treino$Species)
for (i in 1:3){
  for (j in 1:4){
    qqnorm(treino[,j][treino$Species==species[i]])
    qqline(treino[,j][treino$Species==species[i]], col="red")
  }
}
```

Somente a Sepal.Width da espécie setosa que não apresenta normalidade. Mas considerando que avaliamos 12 conjuntos de variáveis (3 espécies e 4 variáveis independentes) e somente 1 mostrou algum desvio, tudo bem se seguirmos.

Teríamos problemas maiores se todas ou maior parte das variáveis para 1 espécie não mostrasse distribuição normal ou se uma ou mais variáveis não mostrarem distribuição normal para todas as espécies. Não é o nosso caso

B. Linearidade de relações
Faça uma verificação visual das relações entre as variáveis, para ver se alguma das relações mostra-se com alguma relação nao linear (p.ex: quadrática, exponencial, eté. Pessoalmente, gosto de fazer essa análise visual com o pacote GGally , muito similar ao ggplot2:

```{r}
GGally::ggpairs(treino, aes(colour = Species, alpha = 0.4))+theme_bw()
```

C. Falta de multicolinearidade entre variáveis independentes
Para isso, é importante verificar a correlação entre as variáveis. Pelo gráfico anterior já pudemos perceber que Petal.Width e  Petal.Length são altamente correlacionadas:

```{r}
cor(treino[,1:4])
```

A qual podemos verificar pela correlação alta = 0.9625962.

Se tivéssemos muitas variáveis, essas duas variáveis poderiam ser sintetizadas em uma única, portanto você poderia utilizar somente  Petal.Width ou Petal.Length, por exemplo. Nós manteremos o uso das duas variáveis, pois o nosso modelo LDA pode perder poder de discriminação se usarmos poucas variávels.

D. Matrizes de dispersão iguais
As Matrizes de variâncias-covariâncias devem ser iguais entre os grupos. Para isso, podemos executar o teste M de Box. Para executarmos uma LDA, amostras pequenas e matrizes de covariância desiguais afetam negativamente a significância do processo de estimação das funções discriminantes.

O pacote biotools possui a função boxM() para que possamos executar esse teste:

```{r}
biotools::boxM(treino[,1:4],treino$Species)
```

Infelizmente o teste M de Box (Boxbs M test) é muito sensível a violações da normalidade, levando a rejeição de grande parte dos casos, que é o nosso caso (p-value <0.01, logo rejeitamos a hipótese nula de que as matrizes são iguais entre os grupos).

Quando as matrizes são diferentes, a sugestão é (i) utilizar QDA ao invés de LDA ou (ii) fazermos uma validação cruzada dos dados, com a separação das amostras emm treino e em teste, como fizemos, e testar se o modelo construído tem um bom ajuste (verificar tabela confusão)

3. Estimação e Avaliação das Funções Discriminantes
No R existem muitos pacotes e maneira de realizar uma lda. Pessoalmente, prefiro a função lda() do pacote MASS, pois tem um output limpo e a forma de escrever a função é muito parecida com um modelo linear (lm() ou glm()):

```{r}
dis1 <- MASS::lda(Species~., data = treino)
dis2 <- MASS::lda(Species~., data = treino, CV=TRUE) # cross-validation
```

com o argumento CV=TRUE, a função lda() executa um cros-validation. A principal diferença de output é que com CV=TRUE há uma tabela no resultado da LDA chamado posterior (dis2$posterior), que são as probabilidades de serem classificadas em cada uma das espécies (Iris setosa, I. versicolor e I. virginica) pela FD (função de Classificação, também chamada de Função Discriminante Linear de Fisher) construída.

Esse objeto posterior é muito semelhante (se a sua FD estiver boa) com o resultado da aplicação da FD nos dados inseridos (predict(dis1)), que também possui uma tabela posterior, com as probabilidades de cada observação pertencer a um determinado grupo (setosa, versicolor ou virginica):

```{r}
previsto <- predict(dis1)
head(round(previsto$posterior,4))
```

Essas probabilidades são obtidas pelos valores das funções discriminantes (escores) obtidas para cada objeto (observação). Esses valores podem ser obtidos na tabela x do objeto previsto (resultante do predict(dis1))

```{r}
escores <- predict(dis1)$x
head(escores)
```

Lembrando que são construídos um número de N???1 funções discriminantes, sendo N o número de grupos existentes. No nosso caso, foram construídos 2 pois temos 3 grupos (espécies): LD1 e LD2.

Os escores dessas funções discriminantes (LD1 e LD2) são importantes para fazer as separações dos grupos. O escore discriminante é  0 para cada um dos LD, portanto, cada grupo é separado pelo seu sinal. A primeira LD (LD1) separa 1 grupo do restante dos grupos. Um objeto será classificado de 1 grupo determinado se apresentar valor positivo ou pertencerá ao restante se apresentar valor negativo. As LDs são sendo construídas até conseguirmos separar todos os grupos do restante. Por isso que o número de LDs será N???1.

O próximo passo é fazer a avalição da precisão preditiva, para isso é construído uma matriz confusão, contrastando o que é previsto pelo modelo e a qual grupo que pertence realmente aquela observação:

```{r}
grupos_previstos <- predict(dis1)$class
grupos_reais <-  treino$Species
tabela<-table(grupos_previstos, grupos_reais)
tabela
```

Para observamos as porcentagens de acertos, verificamos a média da diagonal da matriz confusão:

```{r}
mean(
  diag(tabela)/
  table(grupos_reais)
  )
```

4. Interpretação das Funções Discriminantes
Para isso será necessário ver os escores das cargas discriminantes (correlações das variáveis independentes com as LD -funções discriminantes - construídas). os valores estão contidos no objeto resultante da lda() na tabela scaling:

```{r}
dis1$scaling
```

Portanto, quanto maior o escore de LD1 para um objeto (observação), maior é o comprimento da Sépala, porém menor para a largura da Pétala, pois as correlações são positiva e negativa respectivamente de LD1 com Sepal.Length e Petal.Width.

Para melhor interpretarmos como foi feita a classificação segundo essas funções discriminates, podemos ver os CENTROIDES dos grupos

```{r}
escores<-predict(dis1, treino)$x
resulta<-data.frame(predict(dis1, treino)$x,
                    species = predict(dis1,treino)$class)
centroides<-data.frame(
  LD1=tapply(resulta$LD1, resulta$species, mean),
  LD2=tapply(resulta$LD2, resulta$species, mean))
centroides
```

Ou seja, a primeira LD discriminou setosa do restante (valores positivos pertencem a setosa e valores negativos pertecem às outras duas espécies) e a segunda LD discriminou entre versicolor (valores positivos) e virginica (valores negativos).

Podemos observar com o gráfico dos escores e seus centróides (+) para cada espécie:

```{r}
ggplot(resulta, aes(x=LD1, y=LD2, color=species))+
  geom_point(size=4)+
  stat_ellipse()+
  geom_text(data=centroides, aes(x=LD1+1.5, y=LD2+0.1, label=species), color="black")+
  geom_point(data=centroides, aes(x=LD1, y=LD2), color="black",size=8, shape=3)+
  theme_classic()+
  geom_hline(yintercept = 0)+
  geom_vline(xintercept = 0)
```

Para verificar se as LDs apresentam significância, podemos utilizar o valor de Lambda de Wilk: Testa a significancia estatistica do poder discriminatório da(s) função(ões) discriminante(s). Wilksb lambda: varia de 1.0 (sem poder discriminatorio) ate 0.0 (poder discriminatorio perfeito). A hipótese nula neste caso é que não há diferença significativa entre os centroides dos grupos

```{r}
escores<-predict(
  lda(Species~.,data=treino)
                 )$x # valores de FD para observacao
real <- treino$Species
summary.aov(manova(escores ~ real),test="Wilks")
```

Para ambas funcões (LD1 e LD2) foi significativo, ou seja, os centroides dos grupos são diferentes entre si.

5. Validação Cruzada
Podemos verificar se a nossa amostra teste também responderá com um bom desempenho para a FD construída com os dados de treino (verificando o overfitting):

```{r}
grupos_previstos <- predict(dis1, teste)$class
grupos_reais <-  teste$Species
tabela <- table(grupos_previstos, grupos_reais)
tabela
```

Para observamos as porcentagens de acertos, verificamos a média da diagonal da matriz confusão:

```{r}
mean(
  diag(tabela)/
  table(grupos_reais)
  )
```

6. Conclusões
Qual grupo mostra discriminação clara e significante para a primeira função discriminante?

Quais variáveis independentes mostraram coeficientes (Cargas Fatoriais ou Peso Discriminante) positivos ou negativos com essa primeira função discrimiante?

Qual é a sua conclusão, baseando-se nas duas respostas anteriores? Ou seja, qual espécie apresenta pétalas/sépalas menores/maiores e estreitas/largas?